# -*- coding: utf-8 -*-
"""Churn_Prediction_Robinhood.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jYNZUQc5860qz1YwQPXuHgVbicUC8n_E

### Specify the churned users based on the definition provided by the company.
lets load the datasets:
"""

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold,train_test_split
from sklearn.metrics import classification_report,roc_curve
from sklearn.datasets import make_classification

# Import data from .CSV file
path_equity_value_data = 'Data for robinhood/equity_value_data.csv'
equity_value_data = pd.read_csv(path_equity_value_data)

path_features_data = 'Data for robinhood/features_data.csv'
features_data = pd.read_csv(path_features_data)

# Print out the column types in the equity value dataset
print("Column types in the equity value dataset: \n {}".format(equity_value_data.dtypes))

# Determine the total number of users in the dataset
print("Number of users in the dataset: {}".format(len(equity_value_data.user_id.unique())))

# Change the format of "timestamp" column from string to date
Timestamp_timeformat= pd.to_datetime(equity_value_data['timestamp'],format='%Y-%m-%dT%H:%M:%S%z')
equity_value_data_processed = pd.concat([Timestamp_timeformat,equity_value_data[['close_equity', 'user_id']]],axis=1)

# Print out the column types in the equity value dataset after reformatting
print("Column types in the equity value dataset after reformatting the timestamp column : \n {}".format(equity_value_data_processed.dtypes))

# Print out few rows of reformatted equity value dataset
print("Print few columns of equity value dataset after reformatting the timestamp column :\n")
equity_value_data_processed.head()

# Identify the churned users
churned_user = []
churned_user_id =[]
for ID in equity_value_data_processed.user_id.unique():
    churned_user_id.append(ID)
    Sub_equity_value_data = equity_value_data_processed[equity_value_data_processed.user_id == ID].timestamp.to_numpy()

    for i in range(len(Sub_equity_value_data)):
        
        if i == len(Sub_equity_value_data)-1:
            churned_user.append(0)
            break
        
        elif (Sub_equity_value_data[i+1]-Sub_equity_value_data[i]).days >=28:
            churned_user.append(1)
            break
        
        else:
            pass

# Determine the percentage of churned users
Percentage_of_churned = sum(churned_user)/len(churned_user)
print("what percentage of users have churned in the data provided ? {}".format(Percentage_of_churned*100))

# Create dataframe from the generated lists for churned users and churned users'id.  
churned_user_id_Pdseries = pd.DataFrame(churned_user_id)
churned_user_Pdseries = pd.DataFrame(churned_user)
churned_user_DataFrame = pd.concat([churned_user_id_Pdseries,churned_user_Pdseries], axis=1)
churned_user_DataFrame.columns= ['user_id','churned_status']

# Join the churned dataframe and features_data using user_id column as the key.
Prcessed_Dataframe = features_data.set_index('user_id').join(churned_user_DataFrame.set_index('user_id'))

# Print out few columns of Proccessed_Dataframe
print("Print out few columns of Proccessed_Dataframe after reformatting the timestamp column :\n")
Prcessed_Dataframe.head()

"""### Define function to implement SMOTE method:


"""

# Import the SMOTE from Imblearn library
from imblearn.over_sampling import SMOTE 
def smote_imbal(X,Y):
  ''' 
  The function implements the SMOTE oversampling on the imbalanced dataset.
  
  Input:
  X: feature set
  Y: labels

  Output:
  X_sm: feature set after SMOTE oversampling
  y_sm: labels after SMOTE oversampling
  '''
  smote = SMOTE (sampling_strategy = 'minority')
  X_sm, y_sm = smote.fit_resample(X,Y)
  return X_sm, y_sm

"""### Build the classificaiton model:
Prepare the dataset:
"""

# Create feature set and labels set
X = Prcessed_Dataframe[['risk_tolerance', 'investment_experience', 'liquidity_needs',
       'platform', 'time_spent', 'instrument_type_first_traded',
       'first_deposit_amount', 'time_horizon']]
Y = Prcessed_Dataframe[['churned_status']]

# Determine the columns with categorical variables in X set:
print("Column types in X set: \n {}".format(X.dtypes))

# Determine the unique values in each column of X set:
print("Unique values in each column of X set: \n {}".format(X.nunique()))

# Determine the columns with missing values:
print("Is there any missing value in each column: \n {}".format(X.isnull().any()))

# Perform One-Hot encoding on columns with categorical variables
X_Onehot_cat = pd.get_dummies(X[['risk_tolerance','investment_experience','liquidity_needs','platform','instrument_type_first_traded','time_horizon']])
X_encoded = pd.concat([X_Onehot_cat,X[['time_spent', 'first_deposit_amount']]],axis=1)
# Print out few columns of encoded X set
print("Print out few columns of encoded X set: \n")
X_encoded.head()

# Perform SMOTE oversampeling
X_encoded, Y = smote_imbal(X_encoded, Y)

"""Train and test XGBoost classification model:


"""

# Import XGBoost library
from xgboost import XGBClassifier,plot_importance

# Fit the XGboost classifier 
model = XGBClassifier(max_depth=12, learning_rate=0.01)

# Define evaluation procedure
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)

# Cross validate the model
Y = np.array(Y).ravel()
scores = cross_val_score(model, X_encoded, Y, scoring='roc_auc', cv=cv, n_jobs=-1)

# Print out the mean AUC
print('Mean AUC: %.3f' % np.mean(scores))

"""Determine the top three features based on XGboost model:"""

# Rank the features based on their importance
X_train, X_test, y_train, y_test = train_test_split(X_encoded, Y, test_size=0.2, random_state=40)
model= XGBClassifier(max_depth=5, learning_rate=0.01,scale_pos_weight=13)
xgb_model = model.fit(X_train, y_train)
fig, ax = plt.subplots(figsize=(10,8))
plot_importance(xgb_model, ax=ax);